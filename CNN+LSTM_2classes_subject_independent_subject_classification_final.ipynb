{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safari-mohammadreza/MWL_DeepLearning/blob/main/CNN%2BLSTM_2classes_subject_independent_subject_classification_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL0seVN-WOiP"
      },
      "source": [
        "Code for MohammadReza Safari's Paper\n",
        "Dataset: STEW\n",
        "Analyze in 2classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWzslzxEWy4G"
      },
      "source": [
        "# Initializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd45MULkQJIw"
      },
      "outputs": [],
      "source": [
        "# 1. mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVf95Ru9Rytp"
      },
      "outputs": [],
      "source": [
        "# 2. set directory of data\n",
        "%cd /content/gdrive/My Drive/Thesis/dDTF_images\n",
        "base_dir='/content/gdrive/My Drive/your_path/dDTF_images/2classes'\n",
        "\n",
        "# Define the checkpoint filepath for best model\n",
        "model_checkpoint_filepath = base_dir + '/best_model_2classes_' + freq_band + '_Densenet121LSTM_8folds_fold5' + '.h5'\n",
        "history_dir = base_dir + '/hist_Densenet121LSTM_2classes_SGD_8folds_fold5' + freq_band\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrEa2N_mR0Bw"
      },
      "outputs": [],
      "source": [
        "# 3. import libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from keras.applications.densenet import DenseNet121\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Layer,Dense, Dropout, Input, Activation, TimeDistributed, Reshape\n",
        "from tensorflow.keras.layers import  GRU, Bidirectional\n",
        "from tensorflow.keras.layers import  Conv1D, Conv2D, MaxPooling2D, Flatten, BatchNormalization, LSTM, ZeroPadding2D, GlobalAveragePooling2D, SpatialDropout2D, GlobalMaxPool1D,Convolution1D\n",
        "from tensorflow.keras.callbacks import History\n",
        "from tensorflow.keras.models import Model\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from natsort import natsorted\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import cv2\n",
        "import pickle\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQP-QqwAR1cw"
      },
      "outputs": [],
      "source": [
        "# 4. define some methods\n",
        "\n",
        "def smooth_curve(points, factor=0.2):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "def plot_results(history):\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  %matplotlib inline\n",
        "  plt.plot(epochs, smooth_curve(acc), 'r', alpha = 0.6, label='Training acc')\n",
        "  plt.plot(epochs, smooth_curve(val_acc), 'b', alpha = 0.6, label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, smooth_curve(loss), 'r', alpha = 0.6, label='Training loss')\n",
        "  plt.plot(epochs, smooth_curve(val_loss), 'b', alpha = 0.6, label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# def plot_mean_of_all_folds(train_acc_list, val_acc_list, train_loss_list, val_loss_list):\n",
        "#   # Plot the mean of training and validation accuracy across all folds\n",
        "#   train_acc_mean = np.mean(train_acc_list, axis=0)\n",
        "#   val_acc_mean = np.mean(val_acc_list, axis=0)\n",
        "#   train_loss_mean = np.mean(train_loss_list, axis=0)\n",
        "#   val_loss_mean = np.mean(val_loss_list, axis=0)\n",
        "#   %matplotlib inline\n",
        "#   plt.plot(train_acc_mean, 'r', alpha = 0.6, label='Training Accuracy')\n",
        "#   plt.plot(val_acc_mean, 'b', alpha = 0.6, label='Validation Accuracy')\n",
        "#   plt.title('Training and validation accuracy')\n",
        "#   plt.legend()\n",
        "#   plt.figure()\n",
        "#   plt.plot(train_loss_mean, 'r', alpha = 0.6, label='Training loss')\n",
        "#   plt.plot(val_loss_mean, 'b', alpha = 0.6, label='Validation loss')\n",
        "#   plt.title('Training and validation accuracy')\n",
        "#   plt.legend()\n",
        "#   plt.show()\n",
        "\n",
        "\n",
        "# def calculate_test_accuracy(model, X_test, Y_test):\n",
        "#   fold_results = {}\n",
        "#   Y_test = np_utils.to_categorical(Y_test)\n",
        "#   predicted_probas = model.predict(np.array(X_test),steps = len(X_test))\n",
        "#   fold_results['predicted_probas'] = predicted_probas\n",
        "#   maxindex = predicted_probas[i,:].argmax() ##UnboundLocalError: local variable 'i' referenced before assignment\n",
        "#   binary_prediction=[]\n",
        "#   binary_prediction0=[]\n",
        "#   for i in range(len(predicted_probas)):\n",
        "#      binary_prediction0 = 1 if predicted_probas[i,:].argmax()>=0.5 else 0\n",
        "#     #  binary_prediction0 = np.round(predicted_probas)\n",
        "#      binary_prediction.append(binary_prediction0)\n",
        "#   fold_results['binary_prediction'] = binary_prediction\n",
        "#   conf_mat = confusion_matrix(binary_prediction, Y_test)\n",
        "#   fold_results['confusion_matrix'] = conf_mat\n",
        "#   acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
        "#   fold_results['test_accuracy'] = acc\n",
        "#   print('Overall accuracy: {:.2f} %'.format(acc*100))\n",
        "#   return fold_results\n",
        "\n",
        "def test_accuracy(model, X_test, Y_test):\n",
        "  X_test = np.array(X_test)\n",
        "  Y_test = np.array(Y_test)\n",
        "\n",
        "  Y_test = np_utils.to_categorical(Y_test)\n",
        "  Y_pred = model.predict(X_test)\n",
        "  Y_pred = np.argmax(Y_pred, axis=1)\n",
        "  Y_test = np.argmax(Y_test, axis=1)\n",
        "  test_acc = accuracy_score(Y_pred,Y_test)\n",
        "  cm = confusion_matrix(Y_test, Y_pred)\n",
        "  plot_conf_mat(cm)\n",
        "\n",
        "  return test_acc, cm\n",
        "\n",
        "def plot_conf_mat(cm):\n",
        "  # Normalize the confusion matrix\n",
        "  cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "  conf_labels = ['low', 'high']\n",
        "  sns.heatmap(cm_norm, annot=True, cmap='Blues', fmt='.2%',xticklabels=conf_labels, yticklabels=conf_labels)\n",
        "  plt.xlabel('Actual')\n",
        "  plt.ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "def test_accuracy_v2(model, X_test, Y_test):\n",
        "  X_test = np.array(X_test)\n",
        "  Y_test = np.array(Y_test)\n",
        "\n",
        "  Y_test = to_categorical(Y_test)\n",
        "  Y_pred = model.predict(X_test)\n",
        "  Y_pred = np.argmax(Y_pred, axis=1)\n",
        "  Y_test = np.argmax(Y_test, axis=1)\n",
        "  test_acc = accuracy_score(Y_pred,Y_test)\n",
        "  cm = confusion_matrix(Y_test, Y_pred)\n",
        "  plot_conf_mat_v2(cm)\n",
        "\n",
        "  return test_acc, cm\n",
        "\n",
        "def plot_conf_mat_v2(cm):\n",
        "  conf_labels = ['low', 'high']\n",
        "  sns.heatmap(cm, annot=True, cmap='Blues', fmt='.0f', xticklabels=conf_labels, yticklabels=conf_labels)\n",
        "  plt.xlabel('Actual')\n",
        "  plt.ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "def get_model_metrics(cm):\n",
        "  TP = cm[0, 0]\n",
        "  FP = cm[0, 1]\n",
        "  FN = cm[1, 0]\n",
        "  TN = cm[1, 1]\n",
        "  sensitivity = TP / (TP + FN)\n",
        "  specificity = TN / (TN + FP)\n",
        "  precision = TP / (TP + FP)\n",
        "  FAR = FP / (FP + TN)\n",
        "  FRR = FN / (FN + TP)\n",
        "  print(\"Metrics:\")\n",
        "  print(f\"Sensitivity: {sensitivity:.2f}\")\n",
        "  print(f\"Specificity: {specificity:.2f}\")\n",
        "  print(f\"Precision: {precision:.2f}\")\n",
        "  print(f\"FAR: {FAR:.2f}\")\n",
        "  print(f\"FRR: {FRR:.2f}\")\n",
        "\n",
        "  return sensitivity, specificity, precision, FAR, FRR\n",
        "\n",
        "def create_model():\n",
        "  conv_base = ResNet50(weights='imagenet',include_top=False,input_shape=(224, 224, 3)) # include_top=false because we don't want fullyconnected layers.\n",
        "  # conv_base = DenseNet121(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "  # conv_base = VGG16(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "  # conv_base = EfficientNetB7(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "\n",
        "  conv_base.trainable = True\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Flatten()) # convert output in a array (x,1). voroodi ro migire va saf mikone.\n",
        "  model.add(layers.Dense(32))\n",
        "\n",
        "  cnn = models.Model(model.input, model.output)\n",
        "\n",
        "  seq_input = Input(shape=(5,224,224,3))\n",
        "\n",
        "  encoded_sequence = TimeDistributed(cnn)(seq_input)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=True))(encoded_sequence)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=False))(encoded_sequence)\n",
        "\n",
        "  out = Dense(2, activation=\"softmax\")(encoded_sequence)\n",
        "  #out = Convolution1D(1, kernel_size=1, activation=\"sigmoid\", padding=\"same\")(encoded_sequence)\n",
        "  # out = Dense(1, activation=\"sigmoid\")(encoded_sequence)\n",
        "\n",
        "  cnn_lstm = models.Model(seq_input, out)\n",
        "  cnn_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(learning_rate=1e-4),metrics=['acc'])\n",
        "  # cnn_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4),metrics=['acc'])\n",
        "  # cnn_lstm.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(learning_rate=1e-4),metrics=['acc'])\n",
        "\n",
        "  if fold_count == 1:\n",
        "    cnn_lstm.summary()\n",
        "  return cnn_lstm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-lrsCPwR424"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNNgPMXmR3ng"
      },
      "outputs": [],
      "source": [
        "# 5. load images data from drive\n",
        "# In stew dataset each data has 2.5min or 150sec eeg data and we create dDTF images-\n",
        "# with windowing by size=6sec and step=4sec so 150=6+(n-1)4 --> n=37 images per eeg data\n",
        "\n",
        "# hi_dir = base_dir + '/' + freq_band + '/hi'\n",
        "# hi_data = os.listdir(hi_dir)\n",
        "# hi_data = natsorted(hi_data)\n",
        "# print(np.array(hi_data).shape)\n",
        "\n",
        "# lo_dir = base_dir + '/' + freq_band + '/lo'\n",
        "# lo_data = os.listdir(lo_dir)\n",
        "# lo_data = natsorted(lo_data)\n",
        "# print(np.array(lo_data).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou7gqyUJR8uQ"
      },
      "outputs": [],
      "source": [
        "# 6. Resize images. this cell may take some time\n",
        "\n",
        "# X_q1 = []\n",
        "# for img in lo_data:\n",
        "#     with Image.open(lo_dir+\"/\"+img) as im:\n",
        "#       resized_im = im.resize((224,224))\n",
        "#       X_q1.append(resized_im)\n",
        "# print(np.shape(X_q1))\n",
        "\n",
        "# X_q2 = []\n",
        "# for img in hi_data:\n",
        "#     with Image.open(hi_dir+\"/\"+img) as im:\n",
        "#       resized_im = im.resize((224,224))\n",
        "#       X_q2.append(resized_im)\n",
        "# print(np.shape(X_q2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXmoBWL5R_Rg"
      },
      "outputs": [],
      "source": [
        "# 7. create images_data that is a collection from all classes\n",
        "\n",
        "# Images_data = X_q1 + X_q2 # [lo,hi]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-reJGlDkSAkX"
      },
      "outputs": [],
      "source": [
        "# 8. dump images_data as Images_data.\n",
        "\n",
        "# with open(base_dir+'/Images_data_2classes_' + freq_band , 'wb') as file_pi1:\n",
        "#     pickle.dump(Images_data, file_pi1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1M2ofwESCDH"
      },
      "outputs": [],
      "source": [
        "# 9. load the images_data from drive if you don't wanna run from first cell.\n",
        "print(freq_band)\n",
        "with open(base_dir+'/Images_data_2classes_' + freq_band , 'rb') as f:\n",
        "    Images_data = pickle.load(f)\n",
        "\n",
        "print(np.shape(Images_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKt3C2m7YVJZ"
      },
      "outputs": [],
      "source": [
        "Images_data = [np.asarray(img) for img in Images_data]\n",
        "\n",
        "print(type(Images_data[0]));\n",
        "print(np.shape(Images_data[0]))\n",
        "print(np.shape(Images_data)) # (3552, 224, 224, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdpuMiP8SEpC"
      },
      "source": [
        "Prepare the video samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0LTJcftSD7P"
      },
      "outputs": [],
      "source": [
        "# 11. If you start running from 9th cell you need run this cell to define X_q1 and X_q2\n",
        "\n",
        "X_q1 = Images_data[:37*48]\n",
        "X_q2 = Images_data[37*48:]\n",
        "\n",
        "print(np.shape(X_q1[0]))\n",
        "print(np.shape(X_q1)) # (1776, 224, 224, 3)\n",
        "print(np.shape(X_q2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_q1_test = X_q1[37*24:37*30]\n",
        "X_q2_test = X_q2[37*24:37*30]\n",
        "\n",
        "\n",
        "X_q1 = X_q1[:37*24] + X_q1[37*30:]\n",
        "X_q2 = X_q2[:37*24] + X_q2[37*30:]\n",
        "\n",
        "print(np.shape(X_q1)) # (1554, 224, 224, 3)\n",
        "print(np.shape(X_q2))\n",
        "print(np.shape(X_q1_test)) # (222, 224, 224, 3)\n",
        "print(np.shape(X_q2_test))"
      ],
      "metadata": {
        "id": "KDVzaa1i6HdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSktqt2FtU89"
      },
      "outputs": [],
      "source": [
        "# separate 5 sub's data for test (sub: 1,2,6,7,8)\n",
        "\n",
        "X_q1_test_sub1 = X_q1[:37]\n",
        "X_q2_test_sub1 = X_q2[:37]\n",
        "\n",
        "X_q1_test_sub2 = X_q1[37:37*2]\n",
        "X_q2_test_sub2 = X_q2[37:37*2]\n",
        "\n",
        "X_q1_test_sub6 = X_q1[37*5:37*6]\n",
        "X_q2_test_sub6 = X_q2[37*5:37*6]\n",
        "\n",
        "X_q1_test_sub7 = X_q1[37*6:37*7]\n",
        "X_q2_test_sub7 = X_q2[37*6:37*7]\n",
        "\n",
        "X_q1_test_sub8 = X_q1[37*7:37*8]\n",
        "X_q2_test_sub8 = X_q2[37*7:37*8]\n",
        "\n",
        "X_q1_test = X_q1_test_sub1 + X_q1_test_sub2 + X_q1_test_sub6 + X_q1_test_sub7 + X_q1_test_sub8\n",
        "X_q2_test = X_q2_test_sub1 + X_q2_test_sub2 + X_q2_test_sub6 + X_q2_test_sub7 + X_q2_test_sub8\n",
        "\n",
        "\n",
        "X_q1 = X_q1[37*2:37*5] + X_q1[37*8:]\n",
        "X_q2 = X_q2[37*2:37*5] + X_q2[37*8:]\n",
        "\n",
        "\n",
        "print(np.shape(X_q1)) # (1591, 224, 224, 3)\n",
        "print(np.shape(X_q2))\n",
        "print(np.shape(X_q1_test)) # (185, 224, 224, 3)\n",
        "print(np.shape(X_q2_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_q1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(42):\n",
        "  for img in range(37):\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*43 movie in total.\n",
        "       video_q1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1))\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(42):\n",
        "  for img in range(37):\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2)) # (1386, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q1_test = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(6):\n",
        "  for img in range(37):\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test)) #\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(6):\n",
        "  for img in range(37):\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test)) # (198, 5, 224, 224, 3)\n",
        "print(img)\n"
      ],
      "metadata": {
        "id": "vbYGYmOy7QfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACMebGqwSJcq"
      },
      "outputs": [],
      "source": [
        "# create video data of classes\n",
        "\n",
        "video_q1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(43): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 43 * (37-5+1) = 1419 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*43 movie in total.\n",
        "       video_q1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1)) # The result will be (1419, 5, 224, 224, 3) that is 1419 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(43): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2)) # (1419, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "\n",
        "\n",
        "video_q1_test = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(5): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(5): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDvZg_PtSK1Z"
      },
      "outputs": [],
      "source": [
        "# 14. create video_data that is a collection of all classes and then clear memory\n",
        "\n",
        "video_data = video_q1 + video_q2\n",
        "print(np.shape(video_data)) # (2838, 5, 224, 224, 3)\n",
        "\n",
        "video_data_test = video_q1_test + video_q2_test\n",
        "print(np.shape(video_data_test)) # (330, 5, 224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkRDl4l6iLOq"
      },
      "outputs": [],
      "source": [
        "#/////////////////////////PerSub//////////////////////////////\n",
        "\n",
        "video_q1_test_sub1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub1[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub1)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub1[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub1)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "video_q1_test_sub2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub2[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub2)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub2[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub2)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "video_q1_test_sub6 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub6[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub6.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub6)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub6 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub6[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub6.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub6)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "video_q1_test_sub7 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub7[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub7.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub7)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub7 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub7[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub7.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub7)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "video_q1_test_sub8 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub8[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub8.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub8)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub8 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub8[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub8.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub8)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rOChAfmjfQT"
      },
      "outputs": [],
      "source": [
        "#/////////////////////////PerSub//////////////////////////////\n",
        "video_data_test_sub1 = video_q1_test_sub1 + video_q2_test_sub1\n",
        "print(np.shape(video_data_test_sub1)) # (66, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub2 = video_q1_test_sub2 + video_q2_test_sub2\n",
        "print(np.shape(video_data_test_sub2)) # (66, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub6 = video_q1_test_sub6 + video_q2_test_sub6\n",
        "print(np.shape(video_data_test_sub6)) # (66, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub7 = video_q1_test_sub7 + video_q2_test_sub7\n",
        "print(np.shape(video_data_test_sub7)) # (66, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub8 = video_q1_test_sub8 + video_q2_test_sub8\n",
        "print(np.shape(video_data_test_sub8)) # (66, 5, 224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAfKN8jOkmec"
      },
      "outputs": [],
      "source": [
        "#//////////////////////PerSub/////////////////////////\n",
        "\n",
        "X_test_sub1 = video_data_test_sub1\n",
        "X_test_sub2 = video_data_test_sub2\n",
        "X_test_sub6 = video_data_test_sub6\n",
        "X_test_sub7 = video_data_test_sub7\n",
        "X_test_sub8 = video_data_test_sub8\n",
        "\n",
        "q1_test = 33*[0] #lo\n",
        "q2_test = 33*[1] #hi\n",
        "\n",
        "Y_test = q1_test + q2_test\n",
        "\n",
        "print(np.shape(X_test_sub1))\n",
        "print(np.shape(X_test_sub2))\n",
        "print(np.shape(X_test_sub6))\n",
        "print(np.shape(X_test_sub7))\n",
        "print(np.shape(X_test_sub8))\n",
        "print(np.shape(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. alternative for next cell /////// method: 8folds\n",
        "\n",
        "q1 = 1386*[0] #lo\n",
        "q2 = 1386*[1] #hi\n",
        "\n",
        "labels = q1 + q2 # set label = 0 for 'lo' and label = 1 for 'hi'\n",
        "print(np.shape(labels))\n",
        "\n",
        "q1_test = 198*[0] #lo\n",
        "q2_test = 198*[1] #hi\n",
        "\n",
        "# q1_test = 165*[0] #lo\n",
        "# q2_test = 165*[1] #hi\n",
        "\n",
        "labels_test = q1_test + q2_test # set label = 0 for 'lo' and label = 1 for 'hi'\n",
        "print(np.shape(labels_test))\n"
      ],
      "metadata": {
        "id": "9sf9pNxB78kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnBqaP98SRRp"
      },
      "outputs": [],
      "source": [
        "# 17. labels for video data\n",
        "\n",
        "q1 = 1419*[0] #lo\n",
        "q2 = 1419*[1] #hi\n",
        "\n",
        "labels = q1 + q2 # set label = 0 for 'lo' and label = 1 for 'hi'\n",
        "print(np.shape(labels))\n",
        "\n",
        "q1_test = 165*[0] #lo\n",
        "q2_test = 165*[1] #hi\n",
        "\n",
        "# q1_test = 165*[0] #lo\n",
        "# q2_test = 165*[1] #hi\n",
        "\n",
        "labels_test = q1_test + q2_test # set label = 0 for 'lo' and label = 1 for 'hi'\n",
        "print(np.shape(labels_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKLEXB1hyZSR"
      },
      "source": [
        "# Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlVw5fEvSS1X"
      },
      "outputs": [],
      "source": [
        "# 18. splitting train and test video data\n",
        "\n",
        "X_train = video_data\n",
        "X_test = video_data_test\n",
        "Y_train = labels\n",
        "Y_test = labels_test\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(X_test))\n",
        "print(np.shape(Y_train))\n",
        "print(np.shape(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmkH7zv9Z6By"
      },
      "outputs": [],
      "source": [
        "# clear memory\n",
        "\n",
        "q1 = []\n",
        "q2 = []\n",
        "q1_test = []\n",
        "q2_test = []\n",
        "labels = []\n",
        "labels_test = []\n",
        "video_data = []\n",
        "video_data_test = []\n",
        "video_q1 = []\n",
        "video_q2 = []\n",
        "video_q1_test = []\n",
        "video_q2_test = []\n",
        "Images_data = []\n",
        "Images_data_test = []\n",
        "X_q1 = []\n",
        "X_q2 = []\n",
        "X_q1_test = []\n",
        "X_q2_test = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gYjfX0aymyZ"
      },
      "source": [
        "# Create and Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2gqvT35SWCP"
      },
      "outputs": [],
      "source": [
        "# Fit CNN+LSTM model with video data\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initialize empty lists to store the training and validation accuracy\n",
        "# train_acc_list = []\n",
        "# val_acc_list = []\n",
        "# train_loss_list = []\n",
        "# val_loss_list = []\n",
        "fold_count=0\n",
        "hist = {}\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "# Use early stopping to halt the training of the model at the right time based on the validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "for train_index, val_index in skf.split(X_train,Y_train):\n",
        "    fold_count = fold_count + 1\n",
        "\n",
        "    x_train, x_val = np.array(X_train)[train_index], np.array(X_train)[val_index]\n",
        "    y_train, y_val = np.array(Y_train)[train_index], np.array(Y_train)[val_index]\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_val = to_categorical(y_val)\n",
        "\n",
        "    model = create_model()\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train,y_train,\n",
        "        epochs=50,\n",
        "        batch_size=10,  # I got \"ResourceExhaustedError: Graph execution error\" in batch_size=60\n",
        "        validation_data = (x_val,y_val),\n",
        "        shuffle=True,\n",
        "        callbacks=[early_stopping, checkpoint]\n",
        "        )\n",
        "\n",
        "    plot_results(history)\n",
        "\n",
        "    # Load the best model based on the minimum validation loss\n",
        "    best_model = load_model(model_checkpoint_filepath)\n",
        "\n",
        "    test_acc, conf_mat = test_accuracy_v2(best_model, X_test, Y_test) # this model is trained model (after fitting).\n",
        "\n",
        "    print('fold_'+str(fold_count)+' test_acc: '+str(test_acc))\n",
        "    print('fold_'+str(fold_count)+' confusion matrix: ')\n",
        "    print(conf_mat)\n",
        "    print()\n",
        "\n",
        "    hist['fold'+str(fold_count)+'_history'] = history.history\n",
        "    hist['fold'+str(fold_count)+'_results'] = test_acc\n",
        "\n",
        "    # # save lists of val_acc, acc, val_loss and loss for plotting mean_of_all_folds\n",
        "    # train_acc_list.append(history.history['acc'])\n",
        "    # val_acc_list.append(history.history['val_acc'])\n",
        "\n",
        "    # train_loss_list.append(history.history['loss'])\n",
        "    # val_loss_list.append(history.history['val_loss'])\n",
        "\n",
        "    get_model_metrics(conf_mat)\n",
        "\n",
        "    # save history of each fold\n",
        "    with open(history_dir + '_fold_'+str(fold_count), 'wb') as file_pi1:\n",
        "        pickle.dump(hist, file_pi1)\n",
        "\n",
        "    break\n",
        "# plot_mean_of_all_folds(train_acc_list, val_acc_list, train_loss_list, val_loss_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and test with test data\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(model_checkpoint_filepath)\n",
        "best_model = load_model(model_checkpoint_filepath)"
      ],
      "metadata": {
        "id": "MAErfuwFTOb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, conf_mat = test_accuracy_v2(best_model, X_test, Y_test)\n",
        "\n",
        "print(' test_acc: ' +str(test_acc))\n",
        "print(' confusion matrix: ')\n",
        "print(conf_mat)\n",
        "print()\n",
        "\n",
        "get_model_metrics(conf_mat)\n",
        "print('**********')"
      ],
      "metadata": {
        "id": "prWi7J65CKWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXsPGRBnTTb6"
      },
      "outputs": [],
      "source": [
        "#/////////////PerSub//////////////////////////\n",
        "# Load the best model based on the minimum validation loss\n",
        "print(model_checkpoint_filepath)\n",
        "best_model = load_model(model_checkpoint_filepath)\n",
        "\n",
        "\n",
        "for i in range(1,6):\n",
        "  if i == 1:\n",
        "    X_test = X_test_sub1\n",
        "  if i == 2:\n",
        "    X_test = X_test_sub2\n",
        "  if i == 3:\n",
        "    X_test = X_test_sub6\n",
        "  if i == 4:\n",
        "    X_test = X_test_sub7\n",
        "  if i == 5:\n",
        "    X_test = X_test_sub8\n",
        "\n",
        "  test_acc, conf_mat = test_accuracy(best_model, X_test, Y_test)\n",
        "\n",
        "  print(str(i) + ' test_acc: ' +str(test_acc))\n",
        "  print(' confusion matrix: ')\n",
        "  print(conf_mat)\n",
        "  print()\n",
        "\n",
        "  get_model_metrics(conf_mat)\n",
        "  print('**********')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}