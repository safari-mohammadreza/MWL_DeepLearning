{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL0seVN-WOiP"
      },
      "source": [
        "Code for MohammadReza Safari's Thesis\n",
        "Dataset: STEW\n",
        "Analyze in 2classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWzslzxEWy4G"
      },
      "source": [
        "# Initializer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code has been run on google colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd45MULkQJIw",
        "outputId": "252ab4a9-b2b8-460b-e4f1-b0432e35ef7f"
      },
      "outputs": [],
      "source": [
        "# 1. mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVf95Ru9Rytp",
        "outputId": "7b62b793-6f98-4759-b9e6-200c2864cb80"
      },
      "outputs": [],
      "source": [
        "# 2. set directory of data\n",
        "%cd /content/gdrive/My Drive/Thesis/dDTF_images\n",
        "base_dir='/content/gdrive/My Drive/Thesis/dDTF_images/2classes'\n",
        "\n",
        "freq_band = 'beta'\n",
        "\n",
        "# Define the checkpoint filepath for best model\n",
        "model_checkpoint_filepath = base_dir + '/best_model_2classes_' + freq_band + '_Resnet50LSTM' + '.h5'\n",
        "history_dir = base_dir + '/hist_Resnet50LSTM_2classes_SGD_' + freq_band\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrEa2N_mR0Bw"
      },
      "outputs": [],
      "source": [
        "# 3. import libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from keras.applications.efficientnet import EfficientNetB0\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Layer,Dense, Dropout, Input, Activation, TimeDistributed, Reshape\n",
        "from tensorflow.keras.layers import  GRU, Bidirectional\n",
        "from tensorflow.keras.layers import  Conv1D, Conv2D, MaxPooling2D, Flatten, BatchNormalization, LSTM, ZeroPadding2D, GlobalAveragePooling2D, SpatialDropout2D, GlobalMaxPool1D,Convolution1D\n",
        "from tensorflow.keras.callbacks import History\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils import np_utils\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from natsort import natsorted\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import cv2\n",
        "import pickle\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQP-QqwAR1cw"
      },
      "outputs": [],
      "source": [
        "# 4. define some methods\n",
        "\n",
        "def smooth_curve(points, factor=0.2):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "def plot_results(history):\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  %matplotlib inline\n",
        "  plt.plot(epochs, smooth_curve(acc), 'r', alpha = 0.6, label='Training acc')\n",
        "  plt.plot(epochs, smooth_curve(val_acc), 'b', alpha = 0.6, label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, smooth_curve(loss), 'r', alpha = 0.6, label='Training loss')\n",
        "  plt.plot(epochs, smooth_curve(val_loss), 'b', alpha = 0.6, label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# def plot_mean_of_all_folds(train_acc_list, val_acc_list, train_loss_list, val_loss_list):\n",
        "#   # Plot the mean of training and validation accuracy across all folds\n",
        "#   train_acc_mean = np.mean(train_acc_list, axis=0)\n",
        "#   val_acc_mean = np.mean(val_acc_list, axis=0)\n",
        "#   train_loss_mean = np.mean(train_loss_list, axis=0)\n",
        "#   val_loss_mean = np.mean(val_loss_list, axis=0)\n",
        "#   %matplotlib inline\n",
        "#   plt.plot(train_acc_mean, 'r', alpha = 0.6, label='Training Accuracy')\n",
        "#   plt.plot(val_acc_mean, 'b', alpha = 0.6, label='Validation Accuracy')\n",
        "#   plt.title('Training and validation accuracy')\n",
        "#   plt.legend()\n",
        "#   plt.figure()\n",
        "#   plt.plot(train_loss_mean, 'r', alpha = 0.6, label='Training loss')\n",
        "#   plt.plot(val_loss_mean, 'b', alpha = 0.6, label='Validation loss')\n",
        "#   plt.title('Training and validation accuracy')\n",
        "#   plt.legend()\n",
        "#   plt.show()\n",
        "\n",
        "\n",
        "# def calculate_test_accuracy(model, X_test, Y_test):\n",
        "#   fold_results = {}\n",
        "#   Y_test = np_utils.to_categorical(Y_test)\n",
        "#   predicted_probas = model.predict(np.array(X_test),steps = len(X_test))\n",
        "#   fold_results['predicted_probas'] = predicted_probas\n",
        "#   maxindex = predicted_probas[i,:].argmax() ##UnboundLocalError: local variable 'i' referenced before assignment\n",
        "#   binary_prediction=[]\n",
        "#   binary_prediction0=[]\n",
        "#   for i in range(len(predicted_probas)):\n",
        "#      binary_prediction0 = 1 if predicted_probas[i,:].argmax()>=0.5 else 0\n",
        "#     #  binary_prediction0 = np.round(predicted_probas)\n",
        "#      binary_prediction.append(binary_prediction0)\n",
        "#   fold_results['binary_prediction'] = binary_prediction\n",
        "#   conf_mat = confusion_matrix(binary_prediction, Y_test)\n",
        "#   fold_results['confusion_matrix'] = conf_mat\n",
        "#   acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
        "#   fold_results['test_accuracy'] = acc\n",
        "#   print('Overall accuracy: {:.2f} %'.format(acc*100))\n",
        "#   return fold_results\n",
        "\n",
        "def test_accuracy(model, X_test, Y_test):\n",
        "  X_test = np.array(X_test)\n",
        "  Y_test = np.array(Y_test)\n",
        "\n",
        "  Y_test = np_utils.to_categorical(Y_test)\n",
        "  Y_pred = model.predict(X_test)\n",
        "  Y_pred = np.argmax(Y_pred, axis=1)\n",
        "  Y_test = np.argmax(Y_test, axis=1)\n",
        "  test_acc = accuracy_score(Y_pred,Y_test)\n",
        "  cm = confusion_matrix(Y_test, Y_pred)\n",
        "  plot_conf_mat(cm)\n",
        "\n",
        "  return test_acc, cm\n",
        "\n",
        "def plot_conf_mat(cm):\n",
        "  # Normalize the confusion matrix\n",
        "  cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "  conf_labels = ['low', 'high']\n",
        "  sns.heatmap(cm_norm, annot=True, cmap='Blues', fmt='.2%',xticklabels=conf_labels, yticklabels=conf_labels)\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_model_metrics(cm):\n",
        "  TP = cm[0, 0]\n",
        "  FP = cm[0, 1]\n",
        "  FN = cm[1, 0]\n",
        "  TN = cm[1, 1]\n",
        "  sensitivity = TP / (TP + FN)\n",
        "  specificity = TN / (TN + FP)\n",
        "  precision = TP / (TP + FP)\n",
        "  FAR = FP / (FP + TN)\n",
        "  FRR = FN / (FN + TP)\n",
        "  print(\"Metrics:\")\n",
        "  print(f\"Sensitivity: {sensitivity:.2f}\")\n",
        "  print(f\"Specificity: {specificity:.2f}\")\n",
        "  print(f\"Precision: {precision:.2f}\")\n",
        "  print(f\"FAR: {FAR:.2f}\")\n",
        "  print(f\"FRR: {FRR:.2f}\")\n",
        "\n",
        "  return sensitivity, specificity, precision, FAR, FRR\n",
        "\n",
        "\n",
        "\n",
        "def create_model():\n",
        "  conv_base = ResNet50(weights='imagenet',include_top=False,input_shape=(224, 224, 3)) # include_top=false because we don't want fullyconnected layers.\n",
        "  # conv_base = VGG16(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "  # conv_base = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "\n",
        "  conv_base.trainable = True\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Flatten()) # convert output in a array (x,1). voroodi ro migire va saf mikone.\n",
        "  model.add(layers.Dense(32)) # voroodi ro map mikne roye 32 neuron.\n",
        "\n",
        "  cnn = models.Model(model.input, model.output) # samte rast daqiqn barabar ba 'model' yani mitonest injori bashe: cnn = model. in dastoore models.Model vase vaqtai khobe ke bekhay be laye hat name bedi va az vasataye model khorooji bgiri daron soorat jaye model.output mizari model.[name]\n",
        "  # dim of output: [number of input, 32]\n",
        "\n",
        "  seq_input = Input(shape=(5,224,224,3)) # yani 5 ta akse 224*224*3 ro gozashtim kenare ham va vorodi midim.\n",
        "\n",
        "  encoded_sequence = TimeDistributed(cnn)(seq_input) # TimeDistributed miad voroodi ro (5,224,224,3) migire mikne 5 ta (224,224,3) va mide be cnn va result ro dar nahayat concat mikne yani mishe [5, 32] ke in 32 khoroojie modele cnn'e.\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=True))(encoded_sequence) # 32 ta unit dare va chon bidirectionale pas dotarafast pas mishe 64 ta khorooji. return_sequences=true yani kole sequence haye voroodi ro bargardoon pas khorooji mishe [5, 64]\n",
        "  # encoded_sequence = Dropout(rate=0.1)(encoded_sequence)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=False))(encoded_sequence) # vaqti return_sequences=false mishe yani faqat akharin unit ro mide pas khorooji mishe: [64]\n",
        "\n",
        "  out = Dense(2, activation=\"softmax\")(encoded_sequence)\n",
        "  #out = Convolution1D(1, kernel_size=1, activation=\"sigmoid\", padding=\"same\")(encoded_sequence)\n",
        "  # out = Dense(1, activation=\"sigmoid\")(encoded_sequence)\n",
        "\n",
        "  cnn_lstm = models.Model(seq_input, out)\n",
        "  cnn_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(learning_rate=1e-4),metrics=['acc'])\n",
        "  # cnn_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4),metrics=['acc'])\n",
        "  # cnn_lstm.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(learning_rate=1e-4),metrics=['acc'])\n",
        "\n",
        "  if fold_count == 1:\n",
        "    cnn_lstm.summary()\n",
        "  return cnn_lstm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-lrsCPwR424"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNNgPMXmR3ng",
        "outputId": "de8e43b1-43b1-465e-b90a-694e1e212c25"
      },
      "outputs": [],
      "source": [
        "# 5. load images data from drive\n",
        "# In stew dataset each data has 2.5min or 150sec eeg data and we create dDTF images-\n",
        "# with windowing by size=6sec and step=4sec so 150=6+(n-1)4 --> n=37 images per eeg data\n",
        "\n",
        "# hi_dir = base_dir + '/' + freq_band + '/hi'\n",
        "# hi_data = os.listdir(hi_dir)\n",
        "# hi_data = natsorted(hi_data)\n",
        "# print(np.array(hi_data).shape)\n",
        "\n",
        "# lo_dir = base_dir + '/' + freq_band + '/lo'\n",
        "# lo_data = os.listdir(lo_dir)\n",
        "# lo_data = natsorted(lo_data)\n",
        "# print(np.array(lo_data).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou7gqyUJR8uQ",
        "outputId": "c78c1e79-37a6-4f12-88b8-1b8dcb936f7d"
      },
      "outputs": [],
      "source": [
        "# 6. Resize images. this cell may take some time\n",
        "\n",
        "# X_q1 = []\n",
        "# for img in lo_data:\n",
        "#     with Image.open(lo_dir+\"/\"+img) as im:\n",
        "#       resized_im = im.resize((224,224))\n",
        "#       X_q1.append(resized_im)\n",
        "# print(np.shape(X_q1))\n",
        "\n",
        "# X_q2 = []\n",
        "# for img in hi_data:\n",
        "#     with Image.open(hi_dir+\"/\"+img) as im:\n",
        "#       resized_im = im.resize((224,224))\n",
        "#       X_q2.append(resized_im)\n",
        "# print(np.shape(X_q2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXmoBWL5R_Rg"
      },
      "outputs": [],
      "source": [
        "# 7. create images_data that is a collection from all classes\n",
        "\n",
        "# Images_data = X_q1 + X_q2 # [lo,hi]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-reJGlDkSAkX"
      },
      "outputs": [],
      "source": [
        "# 8. dump images_data as Images_data.\n",
        "\n",
        "# with open(base_dir+'/Images_data_2classes_' + freq_band , 'wb') as file_pi1:\n",
        "#     pickle.dump(Images_data, file_pi1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1M2ofwESCDH",
        "outputId": "12aad662-87f3-443d-b82d-274294f284e2"
      },
      "outputs": [],
      "source": [
        "# 9. load the images_data from drive if you don't wanna run from first cell.\n",
        "\n",
        "with open(base_dir+'/Images_data_2classes_' + freq_band , 'rb') as f:\n",
        "    Images_data = pickle.load(f)\n",
        "\n",
        "print(np.shape(Images_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKt3C2m7YVJZ",
        "outputId": "1913fdda-5260-4bcb-f912-daad61594a66"
      },
      "outputs": [],
      "source": [
        "Images_data = [np.asarray(img) for img in Images_data]\n",
        "\n",
        "print(type(Images_data[0]));\n",
        "print(np.shape(Images_data[0]))\n",
        "print(np.shape(Images_data)) # (3552, 224, 224, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdpuMiP8SEpC"
      },
      "source": [
        "Prepare the video samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0LTJcftSD7P",
        "outputId": "1ddcd0ed-7867-43dc-fa54-e41629b78e72"
      },
      "outputs": [],
      "source": [
        "# 11. If you start running from 9th cell you need run this cell to define X_q1 and X_q2\n",
        "\n",
        "X_q1 = Images_data[:37*48]\n",
        "X_q2 = Images_data[37*48:]\n",
        "\n",
        "print(np.shape(X_q1[0]))\n",
        "print(np.shape(X_q1)) # (1776, 224, 224, 3)\n",
        "print(np.shape(X_q2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSktqt2FtU89",
        "outputId": "6874be96-ac4d-4caf-e2a5-5eb118c3dbf1"
      },
      "outputs": [],
      "source": [
        "# separate 5 sub's data for test (sub: 1,2,6,7,8)\n",
        "\n",
        "X_q1_test_sub1 = X_q1[:37]\n",
        "X_q2_test_sub1 = X_q2[:37]\n",
        "\n",
        "X_q1_test_sub2 = X_q1[37:37*2]\n",
        "X_q2_test_sub2 = X_q2[37:37*2]\n",
        "\n",
        "X_q1_test_sub6 = X_q1[37*5:37*6]\n",
        "X_q2_test_sub6 = X_q2[37*5:37*6]\n",
        "\n",
        "X_q1_test_sub7 = X_q1[37*6:37*7]\n",
        "X_q2_test_sub7 = X_q2[37*6:37*7]\n",
        "\n",
        "X_q1_test_sub8 = X_q1[37*7:37*8]\n",
        "X_q2_test_sub8 = X_q2[37*7:37*8]\n",
        "\n",
        "X_q1_test = X_q1_test_sub1 + X_q1_test_sub2 + X_q1_test_sub6 + X_q1_test_sub7 + X_q1_test_sub8\n",
        "X_q2_test = X_q2_test_sub1 + X_q2_test_sub2 + X_q2_test_sub6 + X_q2_test_sub7 + X_q2_test_sub8\n",
        "\n",
        "\n",
        "X_q1 = X_q1[37*2:37*5] + X_q1[37*8:]\n",
        "X_q2 = X_q2[37*2:37*5] + X_q2[37*8:]\n",
        "\n",
        "\n",
        "print(np.shape(X_q1)) # (1591, 224, 224, 3)\n",
        "print(np.shape(X_q2))\n",
        "print(np.shape(X_q1_test)) # (185, 224, 224, 3)\n",
        "print(np.shape(X_q2_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACMebGqwSJcq",
        "outputId": "db6ac406-8356-434f-c197-557cea09cced"
      },
      "outputs": [],
      "source": [
        "# 12. create video data of classes\n",
        "\n",
        "video_q1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(43): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 43 * (37-5+1) = 1419 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*43 movie in total.\n",
        "       video_q1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1)) # The result will be (1419, 5, 224, 224, 3) that is 1419 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(43): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2)) # (1419, 5, 224, 224, 3)\n",
        "print(img)\n",
        "\n",
        "\n",
        "\n",
        "video_q1_test = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(5): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(5): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkRDl4l6iLOq",
        "outputId": "d5880595-9fc0-4c6f-aaee-1ec1f7309e32"
      },
      "outputs": [],
      "source": [
        "#/////////////////////////PerSub//////////////////////////////\n",
        "\n",
        "video_q1_test_sub1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub1[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub1)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub1[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub1.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub1)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "#///////////////////////////////////////\n",
        "video_q1_test_sub2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub2[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub2)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub2[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub2.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub2)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "#///////////////////////////////////////\n",
        "video_q1_test_sub6 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub6[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub6.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub6)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub6 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub6[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub6.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub6)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "#///////////////////////////////////////\n",
        "video_q1_test_sub7 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub7[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub7.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub7)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub7 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub7[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub7.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub7)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n",
        "#///////////////////////////////////////\n",
        "video_q1_test_sub8 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5 # 5 images per video\n",
        "for subj in range(1): # number of subjects in class = 'lo'\n",
        "  for img in range(37): #stride 1/number of images of each subject / / 5 * (37-5+1) = 165 total video samples per freq. band (ie. gama)\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q1_test_sub8[subj*37+img:subj*37+img+frame_length]\n",
        "       # first 0-5 second 1-6 third 2-7 last 32-37\n",
        "       # 33 movies for each patient. 33*5 movie in total.\n",
        "       video_q1_test_sub8.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q1_test_sub8)) # The result will be (165, 5, 224, 224, 3) that is 165 videos that each of them consist of 5 frames of (224,224,3) images\n",
        "print(img)\n",
        "\n",
        "\n",
        "video_q2_test_sub8 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 5\n",
        "for subj in range(1): # number of subjects\n",
        "  for img in range(37): #stride 1/number of images of each subject\n",
        "       if img+frame_length>37:\n",
        "          break\n",
        "       tmp_video = X_q2_test_sub8[subj*37+img:subj*37+img+frame_length]\n",
        "       video_q2_test_sub8.append(tmp_video)\n",
        "\n",
        "print(np.shape(video_q2_test_sub8)) # (165, 5, 224, 224, 3)\n",
        "print(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rOChAfmjfQT",
        "outputId": "698b7167-3dc7-4208-9bb8-74d77794b8d0"
      },
      "outputs": [],
      "source": [
        "#/////////////////////////PerSub//////////////////////////////\n",
        "video_data_test_sub1 = video_q1_test_sub1 + video_q2_test_sub1\n",
        "print(np.shape(video_data_test_sub1)) # (330, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub2 = video_q1_test_sub2 + video_q2_test_sub2\n",
        "print(np.shape(video_data_test_sub2)) # (330, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub6 = video_q1_test_sub6 + video_q2_test_sub6\n",
        "print(np.shape(video_data_test_sub6)) # (330, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub7 = video_q1_test_sub7 + video_q2_test_sub7\n",
        "print(np.shape(video_data_test_sub7)) # (330, 5, 224, 224, 3)\n",
        "#/////////////////////////\n",
        "video_data_test_sub8 = video_q1_test_sub8 + video_q2_test_sub8\n",
        "print(np.shape(video_data_test_sub8)) # (330, 5, 224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDvZg_PtSK1Z",
        "outputId": "7caa8947-fc49-4dcc-ce2d-40ddf9cee668"
      },
      "outputs": [],
      "source": [
        "# 14. create video_data that is a collection of all classes and then clear memory\n",
        "\n",
        "video_data = video_q1 + video_q2\n",
        "print(np.shape(video_data)) # (2838, 5, 224, 224, 3)\n",
        "\n",
        "video_data_test = video_q1_test + video_q2_test\n",
        "print(np.shape(video_data_test)) # (330, 5, 224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ZU1UCuSOgJ"
      },
      "outputs": [],
      "source": [
        "# 15. dump video_classes as video_data\n",
        "\n",
        "# with open(base_dir+'/video_data_2classes_' + freq_band + '_train', 'wb') as file_pi1:\n",
        "#     pickle.dump(video_data, file_pi1)\n",
        "\n",
        "# with open(base_dir+'/video_data_2classes_' + freq_band + '_test', 'wb') as file_pi1:\n",
        "#     pickle.dump(video_data_test, file_pi1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aJgk3sKSQNn",
        "outputId": "9b4e0240-b8e7-44b9-b913-3ef5a35daa5c"
      },
      "outputs": [],
      "source": [
        "# 16. load video_classes from drive\n",
        "\n",
        "# with open(base_dir+'/video_data_2classes_' + freq_band + '_train', 'rb') as f:\n",
        "#     video_data = pickle.load(f)\n",
        "\n",
        "# print(np.shape(video_data)) # (2838, 5, 224, 224, 3)\n",
        "\n",
        "# with open(base_dir+'/video_data_2classes_' + freq_band + '_test', 'rb') as f:\n",
        "#     video_data_test = pickle.load(f)\n",
        "\n",
        "# print(np.shape(video_data_test)) # (330, 5, 224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnBqaP98SRRp",
        "outputId": "41eeafb1-001a-462c-934a-cd7d3b2cba1b"
      },
      "outputs": [],
      "source": [
        "# 17. labels for video data\n",
        "\n",
        "q1 = 1419*[0] #lo\n",
        "q2 = 1419*[1] #hi\n",
        "\n",
        "labels = q1 + q2 # set label = 0 for 'lo' and label = 1 for 'hi'\n",
        "print(np.shape(labels))\n",
        "\n",
        "q1_test = 165*[0] #lo\n",
        "q2_test = 165*[1] #hi\n",
        "\n",
        "# q1_test = 165*[0] #lo\n",
        "# q2_test = 165*[1] #hi\n",
        "\n",
        "labels_test = q1_test + q2_test # set label = 0 for 'lo' and label = 1 for 'hi'\n",
        "print(np.shape(labels_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKLEXB1hyZSR"
      },
      "source": [
        "# Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAfKN8jOkmec",
        "outputId": "8c1a4828-49d0-4c26-ccbb-6081c226ebcf"
      },
      "outputs": [],
      "source": [
        "#//////////////////////PerSub/////////////////////////\n",
        "\n",
        "X_test_sub1 = video_data_test_sub1\n",
        "X_test_sub2 = video_data_test_sub2\n",
        "X_test_sub6 = video_data_test_sub6\n",
        "X_test_sub7 = video_data_test_sub7\n",
        "X_test_sub8 = video_data_test_sub8\n",
        "\n",
        "q1_test = 33*[0] #lo\n",
        "q2_test = 33*[1] #hi\n",
        "\n",
        "Y_test = q1_test + q2_test\n",
        "\n",
        "print(np.shape(X_test_sub1))\n",
        "print(np.shape(X_test_sub2))\n",
        "print(np.shape(X_test_sub6))\n",
        "print(np.shape(X_test_sub7))\n",
        "print(np.shape(X_test_sub8))\n",
        "print(np.shape(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlVw5fEvSS1X",
        "outputId": "a4047c8a-bc54-4ae1-8a9f-9a34d6a27053"
      },
      "outputs": [],
      "source": [
        "# 18. splitting train and test video data\n",
        "\n",
        "X_train = video_data\n",
        "X_test = video_data_test\n",
        "Y_train = labels\n",
        "Y_test = labels_test\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(X_test))\n",
        "print(np.shape(Y_train))\n",
        "print(np.shape(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmkH7zv9Z6By"
      },
      "outputs": [],
      "source": [
        "# clear memory\n",
        "\n",
        "q1 = []\n",
        "q2 = []\n",
        "q1_test = []\n",
        "q2_test = []\n",
        "labels = []\n",
        "labels_test = []\n",
        "video_data = []\n",
        "video_data_test = []\n",
        "video_q1 = []\n",
        "video_q2 = []\n",
        "video_q1_test = []\n",
        "video_q2_test = []\n",
        "Images_data = []\n",
        "Images_data_test = []\n",
        "X_q1 = []\n",
        "X_q2 = []\n",
        "X_q1_test = []\n",
        "X_q2_test = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gYjfX0aymyZ"
      },
      "source": [
        "# Create and Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K2gqvT35SWCP",
        "outputId": "11ef16de-8da3-4691-fa71-72a8bb4c890f"
      },
      "outputs": [],
      "source": [
        "# Fit CNN+LSTM model with video data\n",
        "\n",
        "# Initialize empty lists to store the training and validation accuracy\n",
        "# train_acc_list = []\n",
        "# val_acc_list = []\n",
        "# train_loss_list = []\n",
        "# val_loss_list = []\n",
        "fold_count=0\n",
        "hist = {}\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "# Use early stopping to halt the training of the model at the right time based on the validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "for train_index, val_index in skf.split(X_train,Y_train):\n",
        "    fold_count = fold_count + 1\n",
        "\n",
        "    x_train, x_val = np.array(X_train)[train_index], np.array(X_train)[val_index]\n",
        "    y_train, y_val = np.array(Y_train)[train_index], np.array(Y_train)[val_index]\n",
        "    y_train = np_utils.to_categorical(y_train)\n",
        "    y_val = np_utils.to_categorical(y_val)\n",
        "\n",
        "    model = create_model()\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train,y_train,\n",
        "        epochs=50,\n",
        "        batch_size=10,  # I got \"ResourceExhaustedError: Graph execution error\" in batch_size=60\n",
        "        validation_data = (x_val,y_val),\n",
        "        shuffle=True,\n",
        "        callbacks=[early_stopping, checkpoint]\n",
        "        )\n",
        "\n",
        "    plot_results(history)\n",
        "\n",
        "    # Load the best model based on the minimum validation loss\n",
        "    best_model = load_model(model_checkpoint_filepath)\n",
        "\n",
        "    test_acc, conf_mat = test_accuracy(best_model, X_test, Y_test) # this model is trained model (after fitting).\n",
        "\n",
        "    print('fold_'+str(fold_count)+' test_acc: '+str(test_acc))\n",
        "    print('fold_'+str(fold_count)+' confusion matrix: ')\n",
        "    print(conf_mat)\n",
        "    print()\n",
        "\n",
        "    hist['fold'+str(fold_count)+'_history'] = history.history\n",
        "    hist['fold'+str(fold_count)+'_results'] = test_acc\n",
        "\n",
        "    # # save lists of val_acc, acc, val_loss and loss for plotting mean_of_all_folds\n",
        "    # train_acc_list.append(history.history['acc'])\n",
        "    # val_acc_list.append(history.history['val_acc'])\n",
        "\n",
        "    # train_loss_list.append(history.history['loss'])\n",
        "    # val_loss_list.append(history.history['val_loss'])\n",
        "\n",
        "    get_model_metrics(conf_mat)\n",
        "\n",
        "    # save history of each fold\n",
        "    with open(history_dir + '_fold_'+str(fold_count), 'wb') as file_pi1:\n",
        "        pickle.dump(hist, file_pi1)\n",
        "\n",
        "    break\n",
        "# plot_mean_of_all_folds(train_acc_list, val_acc_list, train_loss_list, val_loss_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h018ksthK9qF"
      },
      "outputs": [],
      "source": [
        "# Load the best model based on the minimum validation loss\n",
        "best_model = load_model(model_checkpoint_filepath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "TRgiF6mz-2La",
        "outputId": "c4fec0c7-bb26-48e7-9e81-18fa42ad9e91"
      },
      "outputs": [],
      "source": [
        "# Load the best model based on the minimum validation loss\n",
        "\n",
        "best_model = load_model(model_checkpoint_filepath)\n",
        "\n",
        "test_acc, conf_mat = test_accuracy(best_model, X_test, Y_test)\n",
        "print(' test_acc: '+str(test_acc))\n",
        "print(' confusion matrix: ')\n",
        "print(conf_mat)\n",
        "print()\n",
        "\n",
        "get_model_metrics(conf_mat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TXsPGRBnTTb6",
        "outputId": "873b3d10-d944-4698-decc-813b825d6d6c"
      },
      "outputs": [],
      "source": [
        "#/////////////PerSub//////////////////////////\n",
        "for i in range(1,6):\n",
        "  if i == 1:\n",
        "    X_test = X_test_sub1\n",
        "  if i == 2:\n",
        "    X_test = X_test_sub2\n",
        "  if i == 3:\n",
        "    X_test = X_test_sub6\n",
        "  if i == 4:\n",
        "    X_test = X_test_sub7\n",
        "  if i == 5:\n",
        "    X_test = X_test_sub8\n",
        "\n",
        "  test_acc, conf_mat = test_accuracy(best_model, X_test, Y_test)\n",
        "\n",
        "  print(str(i) + ' test_acc: ' +str(test_acc))\n",
        "  print(' confusion matrix: ')\n",
        "  print(conf_mat)\n",
        "  print()\n",
        "\n",
        "  get_model_metrics(conf_mat)\n",
        "  print('**********')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0ko1PyVDEI8",
        "outputId": "180f52c3-3f85-4197-9e7c-26b4085e050b"
      },
      "outputs": [],
      "source": [
        "# print(np.shape(Y_pred))\n",
        "\n",
        "# Y_pred = np.argmax(Y_pred, axis=1) # axis=1 to get the predicted class for each sample\n",
        "\n",
        "# print(np.shape(Y_pred))\n",
        "# print(np.shape(Y_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "Gw7TrWbTC0hU",
        "outputId": "adfc5a34-6327-461d-ad2a-906cdd0a7b90"
      },
      "outputs": [],
      "source": [
        "# test_acc = accuracy_score(Y_train, Y_pred)\n",
        "# cm = confusion_matrix(Y_test, Y_pred)\n",
        "# plt.figure()\n",
        "# plot_conf_mat(cm)\n",
        "\n",
        "# print('test_acc: ' + str(test_acc))\n",
        "# print('confusion matrix: ')\n",
        "# print(conf_mat)\n",
        "# print()\n",
        "\n",
        "# get_model_metrics(conf_mat)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
